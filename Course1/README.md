# Course 1: Generative AI and LLMs: Architecture and Data Preparation
Includes:
Differentiation between generative AI architectures and models, such as RNNs, transformers, VAEs, GANs, and diffusion models

LLMs, such as GPT, BERT, BART, and T5, are applied in natural language processing tasks 

Tokenization to preprocess raw text using NLP libraries like NLTK, spaCy, BertTokenizer, and XLNetTokenizer 

An NLP data loader in PyTorch that handles tokenization, numericalization, and padding for text datasets

## ğŸ“š Modules

- [Module 1: Gen AI acrhiecture and overview](./Module1/)
- [Module 2: Data Preparation](./Module2/)

## ğŸ—‚ï¸ Contents by Module

Each module folder includes:
- `lab.md` â€“ Lab exercises and walkthroughs
- `cheatsheet.md` â€“ Key formulas, commands, or concepts
- `notes.md` â€“ Personal learning notes

